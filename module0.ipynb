{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os, getpass\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun\n",
    "from langchain_core.outputs import ChatResult\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get('OPEN_AI_KEY')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ],
   "id": "53760c37cd9ef56d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T23:38:08.994185Z",
     "start_time": "2025-02-22T23:38:08.982179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema import BaseMessage\n",
    "from typing import List, Optional, Any, Dict\n",
    "import requests\n",
    "\n",
    "class ChatOllama(BaseChatModel):\n",
    "    model_name: str = \"deepseek-r1:14b\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 200,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        response = requests.post(f\"{self.base_url}/api/generate\", json=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data.get(\"response\", \"\").strip()\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: Optional[list[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "        def get_role(message: BaseMessage) -> str:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                return \"Human\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                return \"Assistant\"\n",
    "            else:\n",
    "                return \"Unknown\"\n",
    "\n",
    "        prompt = \"\\n\".join([f\"{get_role(message)}: {message.content}\" for message in messages])\n",
    "        output_text = self._call(prompt, stop=stop)\n",
    "        return {\"text\": output_text}\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: Optional[list[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "        raise NotImplementedError(\"Async generation is not implemented for ChatOllama.\")\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "ollama_chat = ChatOllama(model_name=\"deepseek-r1:14b\", temperature=0.0)"
   ],
   "id": "af06bb708990421b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Say Hello\", name=\"Adam\")\n",
    "\n",
    "# Create a message list\n",
    "messages = [msg]\n",
    "\n",
    "#Invoke a model with a list of messages\n",
    "gpt4o_chat.invoke(messages)"
   ],
   "id": "34e803c6ec5f4a3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T23:38:29.284106Z",
     "start_time": "2025-02-22T23:38:12.562198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Hello! Tell me a joke about the beach.\", name=\"Adam\"),\n",
    "    AIMessage(content=\"Is this a response?\", name=\"Ollama\")\n",
    "]\n",
    "result = ollama_chat._generate(messages)\n",
    "print(\"Ollama Chat Response:\", result[\"text\"])"
   ],
   "id": "a18ffd941b11b24",
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 115)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/share/mise/installs/python/3.13.2/lib/python3.13/site-packages/requests/models.py:963\u001B[0m, in \u001B[0;36mResponse.json\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    962\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 963\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcomplexjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mUnicodeDecodeError\u001B[39;00m:\n\u001B[1;32m    965\u001B[0m     \u001B[38;5;66;03m# Wrong UTF codec detected; usually because it's not UTF-8\u001B[39;00m\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;66;03m# but some other 8-bit codec.  This is an RFC violation,\u001B[39;00m\n\u001B[1;32m    967\u001B[0m     \u001B[38;5;66;03m# and the server didn't bother to tell us what codec *was*\u001B[39;00m\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;66;03m# used.\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/mise/installs/python/3.13.2/lib/python3.13/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/share/mise/installs/python/3.13.2/lib/python3.13/json/decoder.py:348\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n\u001B[0;32m--> 348\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExtra data\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, end)\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Extra data: line 2 column 1 (char 115)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mschema\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HumanMessage, AIMessage\n\u001B[1;32m      3\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      4\u001B[0m     HumanMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello! Tell me a joke about the beach.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdam\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      5\u001B[0m     AIMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIs this a response?\u001B[39m\u001B[38;5;124m\"\u001B[39m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOllama\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m ]\n\u001B[0;32m----> 7\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mollama_chat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOllama Chat Response:\u001B[39m\u001B[38;5;124m\"\u001B[39m, result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "Cell \u001B[0;32mIn[23], line 38\u001B[0m, in \u001B[0;36mChatOllama._generate\u001B[0;34m(self, messages, stop, **kwargs)\u001B[0m\n\u001B[1;32m     35\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     37\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mget_role(message)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmessage\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m message \u001B[38;5;129;01min\u001B[39;00m messages])\n\u001B[0;32m---> 38\u001B[0m output_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_text}\n",
      "Cell \u001B[0;32mIn[23], line 20\u001B[0m, in \u001B[0;36mChatOllama._call\u001B[0;34m(self, prompt, stop)\u001B[0m\n\u001B[1;32m     18\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mpost(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/api/generate\u001B[39m\u001B[38;5;124m\"\u001B[39m, json\u001B[38;5;241m=\u001B[39mpayload)\n\u001B[1;32m     19\u001B[0m response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[0;32m---> 20\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mstrip()\n",
      "File \u001B[0;32m~/.local/share/mise/installs/python/3.13.2/lib/python3.13/site-packages/requests/models.py:971\u001B[0m, in \u001B[0;36mResponse.json\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    969\u001B[0m             \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    970\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 971\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m RequestsJSONDecodeError(e\u001B[38;5;241m.\u001B[39mmsg, e\u001B[38;5;241m.\u001B[39mdoc, e\u001B[38;5;241m.\u001B[39mpos)\n\u001B[1;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m complexjson\u001B[38;5;241m.\u001B[39mloads(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Extra data: line 2 column 1 (char 115)"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
